import numpy as np
import torch
import logging
from dataclasses import dataclass, field
from typing import Dict, Any

from lerobot.robots.robot import Robot
from lerobot.robots.config import RobotConfig
from lerobot.cameras.configs import CameraConfig
from lerobot.cameras.opencv import OpenCVCamera

try:
    from lerobot.robots.mkrobot.follower_mkarm import MKFollower, MKFollowerConfig 
except ImportError:
    # å…è®¸åœ¨æ²¡æœ‰åº•å±‚åº“çš„æœºå™¨ä¸Šè¢«å¯¼å…¥ï¼ˆä¸»è¦é˜²æŠ¥é”™ï¼Œå®é™…è¿è¡Œè¿˜æ˜¯éœ€è¦çš„ï¼‰
    MKFollower = None
    MKFollowerConfig = None

logger = logging.getLogger(__name__)

# --- ç¡¬ä»¶æ–¹å‘ä¿®æ­£ ---
# Sim (URDF) <-> Real (Motor)
HARDWARE_DIR = np.array([-1.0, 1.0, -1.0, -1.0, -1.0, -1.0]) # å‰6è½´

# --- ğŸ›¡ï¸ å®‰å…¨é…ç½®ï¼šç‰©ç†å…³èŠ‚é™ä½ (å•ä½: å¼§åº¦) ---
# è¯·æ ¹æ®æ‚¨çš„ dk2.SLDASM.urdf æ–‡ä»¶ä¸­çš„ limit lower/upper è¿›è¡Œæ ¸å¯¹ä¿®æ­£
# è¿™é‡Œæä¾›çš„æ˜¯ä¸€ç»„ç›¸å¯¹å®‰å…¨çš„é»˜è®¤å€¼
JOINT_LIMITS = {
    # å…³èŠ‚ç´¢å¼•: (æœ€å°å¼§åº¦, æœ€å¤§å¼§åº¦)
    0: (-3.0, 3.0),  # Joint 1: åº•åº§æ—‹è½¬ (é€šå¸¸èŒƒå›´å¾ˆå¤§)
    1: (0.0, 3.0),  # Joint 2: å¤§è‡‚ (æ³¨æ„é¿å…æ’åœ°)
    2: (0.0, 3.0),  # Joint 3: è‚˜éƒ¨
    3: (-1.7, 1.2),  # Joint 4: è…•éƒ¨æ—‹è½¬
    4: (-0.4, 0.4),  # Joint 5: è…•éƒ¨å¼¯æ›²
    5: (-2.0, 2.0),  # Joint 6: æ³•å…°æ—‹è½¬
}

# # çœŸå®æœºæ¢°è‡‚çš„ç‰©ç†é™ä½ (ç”¨äºå‘é€æŒ‡ä»¤å‰çš„å®‰å…¨æˆªæ–­)
# REAL_JOINT_LIMITS = {
#     "joint_1": [-3.0, 3.0],
#     "joint_2": [-0.3, 3.0],
#     "joint_3": [0.0, 3.0],   # æ³¨æ„ï¼šè¿™æ˜¯æ­£å€¼åŒºé—´
#     "joint_4": [-1.7, 1.2],
#     "joint_5": [-0.4, 0.4],  # èŒƒå›´è¾ƒçª„
#     "joint_6": [-2.0, 2.0]
# }

class MKBusAdapter:
    """
    ä¼ªè£…æˆ DynamixelBusï¼Œä¸º gym_manipulator æä¾› sync_read/write æ¥å£ã€‚
    åŒæ—¶ç¡®ä¿å¤ä½æ“ä½œç»è¿‡ MKRobot çš„åæ ‡è½¬æ¢ï¼Œä¿è¯æ–¹å‘å®‰å…¨ã€‚
    """
    def __init__(self, mk_robot):
        self.mk_robot = mk_robot # æŒæœ‰ MKRobot å®ä¾‹ä»¥ä¾¿è°ƒç”¨å…¶ send_action/get_observation
        self.names = ["joint_1", "joint_2", "joint_3", "joint_4", "joint_5", "joint_6", "gripper"]

    @property
    def motors(self):
        # è¿”å›ç”µæœºå­—å…¸ï¼Œç”¨äºè·å–é”®ååˆ—è¡¨
        return self.mk_robot.robot.motors

    def sync_read(self, prop):
        if prop == "Present_Position":
            # ä½¿ç”¨ MKRobot.get_observation è·å–ç»è¿‡ Sim åæ ‡è½¬æ¢åçš„çŠ¶æ€
            obs = self.mk_robot.get_observation()
            state = obs['observation.state'].cpu().numpy()
            
            # å°†æ•°ç»„é‡æ–°æ˜ å°„å›å­—å…¸ {joint_name: value}
            return {name: float(val) for name, val in zip(self.names, state)}
        return {}

    def sync_write(self, prop, values):
        if prop == "Goal_Position":
            # values æ˜¯ {joint_name: val} (Sim åæ ‡ç³»)
            # è½¬æ¢ä¸ºæ•°ç»„å¹¶è°ƒç”¨ MKRobot.send_action (å®ƒä¼šè‡ªåŠ¨å¤„ç† Sim->Real è½¬æ¢)
            target = np.zeros(7, dtype=np.float32)
            for i, name in enumerate(self.names):
                if name in values:
                    target[i] = values[name]
            
            self.mk_robot.send_action(target)

@RobotConfig.register_subclass("mk_robot")
@dataclass
class MKRobotConfig(RobotConfig):
    type: str = "mk_robot"
    port: str = "/dev/ttyACM0"
    joint_velocity_scaling: float = 1.0
    # 0.15 rad â‰ˆ 8.6åº¦ã€‚åœ¨30Hzä¸‹å…è®¸æœ€å¤§è§’é€Ÿåº¦çº¦ 4.5 rad/sã€‚
    # è¿™æ—¢èƒ½è·Ÿä¸Š Reset æŒ‡ä»¤ï¼Œåˆèƒ½é˜²æ­¢ RL ç­–ç•¥è¾“å‡º 3.14 æ—¶çš„é£è½¦äº‹æ•…ã€‚
    max_step_rad: float = 0.15
    cameras: dict[str, CameraConfig] = field(default_factory=dict)

class MKRobot(Robot):
    config_class = MKRobotConfig
    name = "mk_robot"

    def __init__(self, config: MKRobotConfig):
        super().__init__(config)
        self.config = config
        
        #æ‰‹åŠ¨åˆå§‹åŒ–ç›¸æœºåˆ—è¡¨
        self.cameras = {}
        for name, cam_config in config.cameras.items():
            # è¿™é‡Œçš„ cam_config å·²ç»æ˜¯é€šè¿‡ draccus è§£æå¥½çš„é…ç½®å¯¹è±¡
            if cam_config.type == "opencv":
                self.cameras[name] = OpenCVCamera(cam_config)
            else:
                logger.warning(f"âš ï¸ MKRobot ç›®å‰ä»…æ˜¾å¼æ”¯æŒ 'opencv' ç±»å‹ç›¸æœºï¼Œè·³è¿‡: {name} ({cam_config.type})")

        if MKFollowerConfig is None:
            raise ImportError("Could not import follower_mkarm. Please ensure it is in the python path.")

        # åˆå§‹åŒ–åº•å±‚é©±åŠ¨
        self.follower_config = MKFollowerConfig(
            port=config.port,
            joint_velocity_scaling=config.joint_velocity_scaling,
            disable_torque_on_disconnect=True
        )
        self.robot = MKFollower(self.follower_config)
        self.is_connected_flag = False

        self._bus_adapter = MKBusAdapter(self)
        # å®‰å…¨ç›¸å…³ï¼šè®°å½•ä¸Šä¸€æ¬¡çš„ç›®æ ‡ä½ç½®ï¼Œç”¨äºå¹³æ»‘å¤„ç†
        self.last_target_joints = None

    def connect(self):
        if not self.is_connected_flag:
            logger.info(f"ğŸ”— MKRobot: Connecting to {self.config.port}...")
            self.robot.connect()
            #è¿æ¥æ‰€æœ‰æ‘„åƒå¤´
            for name, cam in self.cameras.items():
                logger.info(f"ğŸ“· Connecting camera: {name}")
                cam.connect()

            self.is_connected_flag = True

            # è¿æ¥æ—¶è¯»å–å½“å‰ä½ç½®ä½œä¸ºåˆå§‹ç›®æ ‡ï¼Œé˜²æ­¢ä¸€ä¸Šç”µå°±è·³å˜
            init_obs = self.robot.get_observation()
            if init_obs:
                q_real = np.zeros(6)
                for i in range(6):
                    q_real[i] = init_obs.get(f'joint_{i+1}.pos', 0)
                self.last_target_joints = q_real

            logger.info("âœ… MKRobot: Connected!")

    def disconnect(self):
        if self.is_connected_flag:
            #æ–­å¼€æ‰€æœ‰æ‘„åƒå¤´
            for name, cam in self.cameras.items():
                cam.disconnect()

            self.robot.disconnect()
            self.is_connected_flag = False

    @property
    def is_connected(self):
        return self.is_connected_flag

    @property
    def bus(self):
        # è¿”å›é€‚é…å™¨è€Œä¸æ˜¯åº•å±‚é©±åŠ¨
        return self._bus_adapter

    @property
    def is_calibrated(self):
        """å‡å®šç”µæœºå·²æ ¡å‡†å¥½ï¼Œæˆ–è€…ä¸éœ€è¦æ ¡å‡†"""
        return True

    def calibrate(self):
        """æ ¡å‡†æµç¨‹ï¼ˆç©ºå®ç°ï¼‰"""
        pass

    def configure(self, config):
        """é…ç½®æµç¨‹ï¼ˆç©ºå®ç°ï¼‰"""
        pass

    @property
    def action_features(self):
        """å®šä¹‰åŠ¨ä½œç©ºé—´çš„æ•°æ®ç»“æ„"""
        return {
            "action": {
                "dtype": "float32",
                "shape": (7,),
                "names": ["joint_1", "joint_2", "joint_3", "joint_4", "joint_5", "joint_6", "gripper"],
            }
            }

    @property
    def observation_features(self):
        """å®šä¹‰è§‚æµ‹ç©ºé—´çš„æ•°æ®ç»“æ„"""
        features = {
            "state": {
                "dtype": "float32",
                "shape": (7,),
                "names": ["joint_1", "joint_2", "joint_3", "joint_4", "joint_5", "joint_6", "gripper"],
            },
        }

        # ã€å…³é”®ä¿®æ”¹ã€‘ï¼šåŠ¨æ€æ·»åŠ ç›¸æœºç‰¹å¾
        for cam_name in self.cameras:
            features[f"images.{cam_name}"] = {
                "dtype": "video",
                "shape": (3, self.cameras[cam_name].config.height, self.cameras[cam_name].config.width),
                "names": ["channels", "height", "width"],
            }
        return features

    def capture_images(self) -> Dict[str, Any]:
        """è¯»å–æ‰€æœ‰å·²è¿æ¥æ‘„åƒå¤´çš„å›¾åƒ"""
        images = {}
        for name, camera in self.cameras.items():
            # ä¼˜å…ˆå°è¯•å¼‚æ­¥è¯»å–ä»¥æé«˜å¸§ç‡ï¼Œå¦‚æœä¸æ”¯æŒåˆ™ä½¿ç”¨æ™®é€šè¯»å–
            # if hasattr(camera, "async_read"):
            #     images[name] = camera.async_read()
            # else:
            images[name] = camera.read()
        return images

    # =========================================================
    # ğŸ›¡ï¸ æ ¸å¿ƒå®‰å…¨é€»è¾‘ï¼šé€Ÿåº¦å¹³æ»‘ + ç»å¯¹ä½ç½®ç¡¬é™ä½
    # =========================================================

    def send_action(self, action: torch.Tensor) -> torch.Tensor:
        if not self.is_connected: return action

        # 1. æ ¼å¼è½¬æ¢
        if isinstance(action, dict) and "action" in action:
            q_sim_target_data = action["action"]
        else:
            q_sim_target_data = action

        if isinstance(q_sim_target_data, torch.Tensor):
            q_sim_target = q_sim_target_data.cpu().numpy()
        else:
            q_sim_target = q_sim_target_data

        # 2. æ˜ å°„åˆ° Real åæ ‡ç³»
        q_real_target = q_sim_target[:6] * HARDWARE_DIR
        g_real_target = np.clip(q_sim_target[6], 0.0, 1.0)

        # 3. è¯»å–å½“å‰çœŸå®ä½ç½®
        current_obs = self.robot.get_observation()
        q_real_current = np.zeros(6)
        for i in range(6):
            q_real_current[i] = current_obs.get(f'joint_{i+1}.pos', 0)

        # ---------------------------------------------------
        # ğŸ›¡ï¸ ä¼˜åŒ–åçš„å®‰å…¨é€»è¾‘: å…ˆä½ç½®æˆªæ–­ï¼Œå†é€Ÿåº¦æˆªæ–­
        # ---------------------------------------------------
        
        q_real_safe = np.zeros(6)
        
        for i in range(6):
            # A. è·å–é™ä½
            min_lim, max_lim = JOINT_LIMITS.get(i, (-3.14, 3.14))
            
            # B. ã€å…³é”®ã€‘å…ˆå°†ç›®æ ‡å¼ºè¡Œé™åˆ¶åœ¨ç‰©ç†é™ä½å†…
            # è¿™æ ·æ— è®º Policy æƒ³è¦å»å¤šè¿œçš„åœ°æ–¹ï¼Œæˆ‘ä»¬åªæŠŠå®ƒå½“åšæƒ³è¦å»è¾¹ç•Œ
            target_clamped = np.clip(q_real_target[i], min_lim, max_lim)
            
            # C. è®¡ç®— çœŸå®ä½ç½® -> è¾¹ç•Œ çš„è·ç¦»
            delta = target_clamped - q_real_current[i]
            
            # D. å¯¹è¿™ä¸ªè·ç¦»è¿›è¡Œé™é€Ÿ (å¹³æ»‘å¤„ç†)
            # å³ä½¿ current åœ¨è¾¹ç•Œå¤– (ä¾‹å¦‚ 2.0, limit 1.6), delta æ˜¯ -0.4
            # ä¹Ÿä¼šè¢«å¹³æ»‘é™åˆ¶ä¸º -0.15, ä»è€Œå®‰å…¨åœ°æ…¢æ…¢é€€å›ï¼Œè€Œä¸æ˜¯å‰§çƒˆè·³å˜
            max_step = self.config.max_step_rad
            delta_safe = np.clip(delta, -max_step, max_step)
            
            # E. æœ€ç»ˆæŒ‡ä»¤
            q_real_safe[i] = q_real_current[i] + delta_safe

        # 4. å‘é€æœ€ç»ˆçš„å®‰å…¨æŒ‡ä»¤
        command = {
            "joint_1.pos": q_real_safe[0],
            "joint_2.pos": q_real_safe[1],
            "joint_3.pos": q_real_safe[2],
            "joint_4.pos": q_real_safe[3],
            "joint_5.pos": q_real_safe[4],
            "joint_6.pos": q_real_safe[5],
            "gripper.pos": g_real_target
        }
        self.robot.send_action(command)
        
        return action

    def get_observation(self) -> Dict[str, Any]:
        """è¿”å›çš„æ•°æ®é”®åå¿…é¡»ä¸ä¸Šé¢ observation_features çš„ Key å®Œå…¨ä¸€è‡´"""
        if not self.is_connected:
            return {"state": torch.zeros(7)}

        raw_obs = self.robot.get_observation()
        
        # 1. å¤„ç†å…³èŠ‚æ•°æ®
        q_sim = np.zeros(7)
        for i in range(6):
            q_sim[i] = raw_obs.get(f'joint_{i+1}.pos', 0) * HARDWARE_DIR[i]
        q_sim[6] = raw_obs.get('gripper.pos', 0)
        
        # 2. æ•è·å›¾åƒ
        images = self.capture_images() 

        # 3. ç»„è£…å­—å…¸
        obs_dict = {
            "state": torch.from_numpy(q_sim).float(),
            # å…¼å®¹åº•å±‚ Gym ç¯å¢ƒéœ€è¦çš„åŸå§‹è½´å
            "joint_1.pos": q_sim[0], "joint_2.pos": q_sim[1], "joint_3.pos": q_sim[2],
            "joint_4.pos": q_sim[3], "joint_5.pos": q_sim[4], "joint_6.pos": q_sim[5],
            "gripper.pos": q_sim[6],
        }

        # 4. ç»„è£…å›¾åƒ
        for cam_name, img in images.items():
            obs_dict[f"images.{cam_name}"] = img

        return obs_dict

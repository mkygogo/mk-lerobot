# !/usr/bin/env python

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import time
import os
from dataclasses import dataclass
from typing import Any

import gymnasium as gym
import numpy as np
import torch

from lerobot.cameras import opencv  # noqa: F401
from lerobot.configs import parser
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.envs.configs import HILSerlRobotEnvConfig
from lerobot.model.kinematics import RobotKinematics
from lerobot.processor import (
    AddBatchDimensionProcessorStep,
    AddTeleopActionAsComplimentaryDataStep,
    AddTeleopEventsAsInfoStep,
    DataProcessorPipeline,
    DeviceProcessorStep,
    EnvTransition,
    GripperPenaltyProcessorStep,
    ImageCropResizeProcessorStep,
    InterventionActionProcessorStep,
    JointVelocityProcessorStep,
    MapDeltaActionToRobotActionStep,
    MapTensorToDeltaActionDictStep,
    MotorCurrentProcessorStep,
    Numpy2TorchActionProcessorStep,
    RewardClassifierProcessorStep,
    RobotActionToPolicyActionProcessorStep,
    TimeLimitProcessorStep,
    Torch2NumpyActionProcessorStep,
    TransitionKey,
    VanillaObservationProcessorStep,
    create_transition,
)
from lerobot.processor.converters import identity_transition
from lerobot.robots import (  # noqa: F401
    RobotConfig,
    make_robot_from_config,
    so100_follower,
    so101_follower,
)
from lerobot.robots.robot import Robot
from lerobot.robots.so100_follower.robot_kinematic_processor import (
    EEBoundsAndSafety,
    EEReferenceAndDelta,
    ForwardKinematicsJointsToEEObservation,
    GripperVelocityToJoint,
    InverseKinematicsRLStep,
)
from lerobot.teleoperators import (
    gamepad,  # noqa: F401
    keyboard,  # noqa: F401
    make_teleoperator_from_config,
    so101_leader,  # noqa: F401
    so100_leader,
)
from lerobot.teleoperators.teleoperator import Teleoperator
from lerobot.teleoperators.utils import TeleopEvents
from lerobot.utils.constants import ACTION, DONE, OBS_IMAGES, OBS_STATE, REWARD
from lerobot.utils.robot_utils import busy_wait
from lerobot.utils.utils import log_say

try:
    from lerobot.robots.mkrobot.mk_robot import MKRobotConfig
    from lerobot.teleoperators.gamepad.gamepad_ik_teleop import GamepadIKTeleopConfig
    print("âœ… å·²æˆåŠŸæ³¨å†Œ MKRobot å’Œ GamepadIK")
except ImportError as e:
    print(f"âš ï¸ æ³¨å†Œ MKRobot/GamepadIK å¤±è´¥ (å¦‚æœä¸æ˜¯ç”¨è¿™ä¸¤ä¸ªç¡¬ä»¶å¯å¿½ç•¥): {e}")

#å¯¼å…¥æˆ‘ä»¬åˆšå†™çš„å®‰å…¨å¤„ç†å™¨
# æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰è¿™ä¸ªæ–‡ä»¶ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®Œæˆäº†ä¸Šä¸€æ­¥æ–°å»º safety_processor.py çš„æ“ä½œ
try:
    from lerobot.processor.safety_processor import MKArmSafetyProcessorStep
except ImportError:
    MKArmSafetyProcessorStep = None
    print("âš ï¸ Warning: MKArmSafetyProcessorStep not found. Safety checks will be disabled.")

logging.basicConfig(level=logging.WARNING)
logging.getLogger("lerobot.src.lerobot.rl.learner").setLevel(logging.WARNING)
logging.getLogger("lerobot.src.lerobot.rl.learner_service").setLevel(logging.WARNING)


# å±è”½ MKRobot é©±åŠ¨çš„è¯»å–è€—æ—¶æ—¥å¿— å¯¹åº”æ–‡ä»¶: src/lerobot/robots/mkrobot/follower_mkarm.py
logging.getLogger("lerobot.robots.mkrobot.follower_mkarm").setLevel(logging.INFO)
logging.getLogger("lerobot.robots.mkrobot").setLevel(logging.INFO)

# å±è”½ OpenCV æ‘„åƒå¤´çš„è¯»å–è€—æ—¶æ—¥å¿— å¯¹åº”æ–‡ä»¶: src/lerobot/cameras/opencv/camera_opencv.py
logging.getLogger("lerobot.cameras.opencv.camera_opencv").setLevel(logging.INFO)

# --- ğŸ›¡ï¸ é…ç½®åŒºåŸŸï¼šPolicy å®‰å…¨å±‹ (è®­ç»ƒæ´»åŠ¨èŒƒå›´) ---
# è¿™é‡Œçš„èŒƒå›´åº”è¯¥æ¯” mk_robot.py é‡Œçš„ç‰©ç†ç¡¬é™ä½è¦å° (å»ºè®® 80%~90%)
# ç¡®ä¿ Policy ä¸ä¼šæŠŠæœºæ¢°è‡‚æ‰­æˆ IK ç®—ä¸å‡ºæ¥çš„éº»èŠ±å§¿æ€ï¼Œæ–¹ä¾¿äººå·¥éšæ—¶æ¥ç®¡
POLICY_SAFE_LIMITS = {
    # å…³èŠ‚ç´¢å¼•: (æœ€å°å¼§åº¦, æœ€å¤§å¼§åº¦)
    0: (-1.0, 1.0), # Base
    1: (0.74, 1.70), # Shoulder (é™åˆ¶ä¸è¦å€’åœ°)
    2: (-1.0, -0.42), # Elbow
    3: (-1.7, 1.2), # Wrist 1
    4: (-0.4, 0.4), # Wrist 2
    5: (-2.0, 2.0), # Wrist 3
}

@dataclass
class DatasetConfig:
    """Configuration for dataset creation and management."""

    repo_id: str
    task: str
    root: str | None = None
    num_episodes_to_record: int = 5
    replay_episode: int | None = None
    push_to_hub: bool = False


@dataclass
class GymManipulatorConfig:
    """Main configuration for gym manipulator environment."""

    env: HILSerlRobotEnvConfig
    dataset: DatasetConfig
    mode: str | None = None  # Either "record", "replay", None
    device: str = "cpu"


def reset_follower_position(robot_arm: Robot, target_position: np.ndarray) -> None:
    """Reset robot arm to target position using smooth trajectory."""
    current_position_dict = robot_arm.bus.sync_read("Present_Position")
    current_position = np.array(
        [current_position_dict[name] for name in current_position_dict], dtype=np.float32
    )
    trajectory = torch.from_numpy(
        np.linspace(current_position, target_position, 50)
    )  # NOTE: 30 is just an arbitrary number
    for pose in trajectory:
        action_dict = dict(zip(current_position_dict, pose, strict=False))
        robot_arm.bus.sync_write("Goal_Position", action_dict)
        busy_wait(0.015)


class RobotEnv(gym.Env):
    """Gym environment for robotic control with human intervention support."""

    def __init__(
        self,
        robot,
        use_gripper: bool = False,
        display_cameras: bool = False,
        reset_pose: list[float] | None = None,
        reset_time_s: float = 5.0,
    ) -> None:
        """Initialize robot environment with configuration options.

        Args:
            robot: Robot interface for hardware communication.
            use_gripper: Whether to include gripper in action space.
            display_cameras: Whether to show camera feeds during execution.
            reset_pose: Joint positions for environment reset.
            reset_time_s: Time to wait during reset.
        """
        super().__init__()

        self.robot = robot
        self.display_cameras = display_cameras

        # Connect to the robot if not already connected.
        if not self.robot.is_connected:
            self.robot.connect()

        # Episode tracking.
        self.current_step = 0
        self.episode_data = None

        self._joint_names = [f"{key}.pos" for key in self.robot.bus.motors]
        self._image_keys = self.robot.cameras.keys()

        self.reset_pose = reset_pose
        self.reset_time_s = reset_time_s

        self.use_gripper = use_gripper

        self._joint_names = list(self.robot.bus.motors.keys())
        self._raw_joint_positions = None

        #ç”¨äºå­˜å‚¨ä¸Šä¸€æ­¥çš„å¹³æ»‘åŠ¨ä½œï¼Œå®ç°æ»¤æ³¢
        self.last_policy_action = None

        #çŠ¶æ€æœºå˜é‡
        # æ¨¡å¼: "IDLE" (å‘å‘†/ä¿æŒ), "EXPLORE" (RLæ¢ç´¢), "ZEROING" (è‡ªåŠ¨å½’é›¶)
        self.rl_mode = "IDLE" 

        self.last_policy_action = None # ç”¨äºå¹³æ»‘æ»¤æ³¢

        self._setup_spaces()

    def _get_observation(self) -> dict[str, Any]:
        """Get current robot observation including joint positions and camera images."""
        obs_dict = self.robot.get_observation()
        raw_joint_joint_position = {f"{name}.pos": obs_dict[f"{name}.pos"] for name in self._joint_names}
        joint_positions = np.array([raw_joint_joint_position[f"{name}.pos"] for name in self._joint_names])

        images = {key: obs_dict[key] for key in self._image_keys}

        return {"agent_pos": joint_positions, "pixels": images, **raw_joint_joint_position}

    def _setup_spaces(self) -> None:
        """Configure observation and action spaces based on robot capabilities."""
        current_observation = self._get_observation()

        observation_spaces = {}

        # Define observation spaces for images and other states.
        if current_observation is not None and "pixels" in current_observation:
            prefix = OBS_IMAGES
            observation_spaces = {
                f"{prefix}.{key}": gym.spaces.Box(
                    low=0, high=255, shape=current_observation["pixels"][key].shape, dtype=np.uint8
                )
                for key in current_observation["pixels"]
            }

        if current_observation is not None:
            agent_pos = current_observation["agent_pos"]

            observation_spaces[OBS_STATE] = gym.spaces.Box(
                #low=0,
                #high=10,
                low=-6.28, 
                high=6.28,
                shape=agent_pos.shape,
                dtype=np.float32,
            )

        self.observation_space = gym.spaces.Dict(observation_spaces)

        # Define the action space for joint positions along with setting an intervention flag.
        #  Action Space æ”¹ä¸ºç›´æ¥å¯¹åº”å…³èŠ‚æ•°é‡.è¿™é‡Œæ˜¯æ ¹æ®mkrobotæ”¹æ‰äº†ï¼Œå¯èƒ½ä¸é€‚åº”so101äº†
        #action_dim = 3
        action_dim = len(self._joint_names)

        bounds = {}
        bounds["min"] = -np.ones(action_dim)
        bounds["max"] = np.ones(action_dim)

        ## (åˆ é™¤åŸæœ¬å…³äº use_gripper çš„ if/else åˆ¤æ–­ï¼Œå› ä¸º joint_names é‡Œå·²ç»åŒ…å«äº† gripper)
        # if self.use_gripper:
        #     action_dim += 1
        #     bounds["min"] = np.concatenate([bounds["min"], [0]])
        #     bounds["max"] = np.concatenate([bounds["max"], [2]])

        self.action_space = gym.spaces.Box(
            low=bounds["min"],
            high=bounds["max"],
            shape=(action_dim,),
            dtype=np.float32,
        )

    def reset(
        self, *, seed: int | None = None, options: dict[str, Any] | None = None
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        """Reset environment to initial state.

        Args:
            seed: Random seed for reproducibility.
            options: Additional reset options.

        Returns:
            Tuple of (observation, info) dictionaries.
        """
        #Reset æ—¶ä¸è¦é‡ç½® rl_modeï¼Œä¿æŒç”¨æˆ·çš„æ§åˆ¶çŠ¶æ€
        # æ¯”å¦‚ç”¨æˆ·æ­£åœ¨ EXPLOREï¼Œå›åˆç»“æŸ reset ååº”è¯¥ç»§ç»­ EXPLOREï¼Œä¸éœ€è¦é‡æ–°æŒ‰ Y
        # é™¤éå¤„äºå½’é›¶çŠ¶æ€ï¼Œå½’é›¶å®Œæˆåä¼šè‡ªåŠ¨åˆ‡å› IDLE
        
        start_time = time.perf_counter()
        if self.reset_pose is not None:
            log_say("Reset the environment.", play_sounds=True)
            reset_follower_position(self.robot, np.array(self.reset_pose))
            log_say("Reset the environment done.", play_sounds=True)

        busy_wait(self.reset_time_s - (time.perf_counter() - start_time))
        super().reset(seed=seed, options=options)
        self.current_step = 0
        self.episode_data = None
        
        self.last_policy_action = None
        
        obs = self._get_observation()
        self._raw_joint_positions = {f"{key}.pos": obs[f"{key}.pos"] for key in self._joint_names}
        return obs, {TeleopEvents.IS_INTERVENTION: False}

    def step(self, action) -> tuple[dict[str, np.ndarray], float, bool, bool, dict[str, Any]]:
        # [ä¿®æ­£] ç›´æ¥å°† action æ•°ç»„ä¼ ç»™ Robot
        # MKRobot çš„ send_action æ¥æ”¶æ•°ç»„ï¼Œå¹¶åœ¨å†…éƒ¨å¤„ç† Sim->Real è½¬æ¢å’Œå­—å…¸æ‰“åŒ…
        # ä¹‹å‰çš„ä»£ç æ‰‹åŠ¨æ‰“åŒ…æˆäº†å­—å…¸ï¼Œå¯¼è‡´ MKRobot å†…éƒ¨å¯¹å­—å…¸åˆ‡ç‰‡æŠ¥é”™
        self.robot.send_action(action)

        obs = self._get_observation()
        self._raw_joint_positions = {f"{key}.pos": obs[f"{key}.pos"] for key in self._joint_names}

        if self.display_cameras:
            self.render()

        self.current_step += 1
        reward = 0.0
        terminated = False
        truncated = False

        return (
            obs,
            reward,
            terminated,
            truncated,
            #{TeleopEvents.IS_INTERVENTION: False},
            {},
        )
    # def step(self, action) -> tuple[dict[str, np.ndarray], float, bool, bool, dict[str, Any]]:
    #     """Execute one environment step with given action."""
    #     joint_targets_dict = {f"{key}.pos": action[i] for i, key in enumerate(self.robot.bus.motors.keys())}

    #     self.robot.send_action(joint_targets_dict)

    #     obs = self._get_observation()

    #     self._raw_joint_positions = {f"{key}.pos": obs[f"{key}.pos"] for key in self._joint_names}

    #     if self.display_cameras:
    #         self.render()

    #     self.current_step += 1

    #     reward = 0.0
    #     terminated = False
    #     truncated = False

    #     return (
    #         obs,
    #         reward,
    #         terminated,
    #         truncated,
    #         {TeleopEvents.IS_INTERVENTION: False},
    #     )

    def render(self) -> None:
        """Display robot camera feeds."""
        #import cv2

        current_observation = self._get_observation()
        # if current_observation is not None:
        #     image_keys = [key for key in current_observation if "image" in key]

        #     for key in image_keys:
        #         cv2.imshow(key, cv2.cvtColor(current_observation[key].numpy(), cv2.COLOR_RGB2BGR))
        #         cv2.waitKey(1)

    def close(self) -> None:
        """Close environment and disconnect robot."""
        if self.robot.is_connected:
            self.robot.disconnect()

    def get_raw_joint_positions(self) -> dict[str, float]:
        """Get raw joint positions."""
        return self._raw_joint_positions


def make_robot_env(cfg: HILSerlRobotEnvConfig) -> tuple[gym.Env, Any]:
    """Create robot environment from configuration.

    Args:
        cfg: Environment configuration.

    Returns:
        Tuple of (gym environment, teleoperator device).
    """
    # Check if this is a GymHIL simulation environment
    if cfg.name == "gym_hil":
        assert cfg.robot is None and cfg.teleop is None, "GymHIL environment does not support robot or teleop"
        import gym_hil  # noqa: F401

        # Extract gripper settings with defaults
        use_gripper = cfg.processor.gripper.use_gripper if cfg.processor.gripper is not None else True
        gripper_penalty = cfg.processor.gripper.gripper_penalty if cfg.processor.gripper is not None else 0.0

        env = gym.make(
            f"gym_hil/{cfg.task}",
            image_obs=True,
            render_mode="human",
            use_gripper=use_gripper,
            gripper_penalty=gripper_penalty,
        )

        return env, None

    # Real robot environment
    assert cfg.robot is not None, "Robot config must be provided for real robot environment"
    assert cfg.teleop is not None, "Teleop config must be provided for real robot environment"

    robot = make_robot_from_config(cfg.robot)
    teleop_device = make_teleoperator_from_config(cfg.teleop)
    teleop_device.connect()

    # Create base environment with safe defaults
    use_gripper = cfg.processor.gripper.use_gripper if cfg.processor.gripper is not None else True
    display_cameras = (
        cfg.processor.observation.display_cameras if cfg.processor.observation is not None else False
    )
    reset_pose = cfg.processor.reset.fixed_reset_joint_positions if cfg.processor.reset is not None else None

    env = RobotEnv(
        robot=robot,
        use_gripper=use_gripper,
        display_cameras=display_cameras,
        reset_pose=reset_pose,
    )

    return env, teleop_device


def make_processors(
    env: gym.Env, teleop_device: Teleoperator | None, cfg: HILSerlRobotEnvConfig, device: str = "cpu"
) -> tuple[
    DataProcessorPipeline[EnvTransition, EnvTransition], DataProcessorPipeline[EnvTransition, EnvTransition]
]:
    """Create environment and action processors.

    Args:
        env: Robot environment instance.
        teleop_device: Teleoperator device for intervention.
        cfg: Processor configuration.
        device: Target device for computations.

    Returns:
        Tuple of (environment processor, action processor).
    """
    terminate_on_success = (
        cfg.processor.reset.terminate_on_success if cfg.processor.reset is not None else True
    )

    if cfg.name == "gym_hil":
        action_pipeline_steps = [
            InterventionActionProcessorStep(terminate_on_success=terminate_on_success),
            Torch2NumpyActionProcessorStep(),
        ]

        env_pipeline_steps = [
            Numpy2TorchActionProcessorStep(),
            VanillaObservationProcessorStep(),
            AddBatchDimensionProcessorStep(),
            DeviceProcessorStep(device=device),
        ]

        return DataProcessorPipeline(
            steps=env_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
        ), DataProcessorPipeline(
            steps=action_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
        )

    # Full processor pipeline for real robot environment
    # Get robot and motor information for kinematics
    motor_names = list(env.robot.bus.motors.keys())

    # Set up kinematics solver if inverse kinematics is configured
    kinematics_solver = None
    if cfg.processor.inverse_kinematics is not None:
        kinematics_solver = RobotKinematics(
            urdf_path=cfg.processor.inverse_kinematics.urdf_path,
            target_frame_name=cfg.processor.inverse_kinematics.target_frame_name,
            joint_names=motor_names,
        )

    env_pipeline_steps = [VanillaObservationProcessorStep()]

    if cfg.processor.observation is not None:
        if cfg.processor.observation.add_joint_velocity_to_observation:
            env_pipeline_steps.append(JointVelocityProcessorStep(dt=1.0 / cfg.fps))
        if cfg.processor.observation.add_current_to_observation:
            env_pipeline_steps.append(MotorCurrentProcessorStep(robot=env.robot))

    if kinematics_solver is not None:
        env_pipeline_steps.append(
            ForwardKinematicsJointsToEEObservation(
                kinematics=kinematics_solver,
                motor_names=motor_names,
            )
        )

    if cfg.processor.image_preprocessing is not None:
        env_pipeline_steps.append(
            ImageCropResizeProcessorStep(
                crop_params_dict=cfg.processor.image_preprocessing.crop_params_dict,
                resize_size=cfg.processor.image_preprocessing.resize_size,
            )
        )

    # Add time limit processor if reset config exists
    if cfg.processor.reset is not None:
        # --- ğŸ‘‡ æ–°å¢è°ƒè¯•æ‰“å° ğŸ‘‡ ---
        limit_steps = int(cfg.processor.reset.control_time_s * cfg.fps)
        print(f"\nğŸ›‘ [PROCESSOR CHECK] Creating TimeLimitProcessorStep:")
        print(f"   ğŸ‘‰ Raw control_time_s: {cfg.processor.reset.control_time_s}")
        print(f"   ğŸ‘‰ Raw FPS: {cfg.fps}")
        print(f"   ğŸ‘‰ Final Max Episode Steps: {limit_steps} (Will run for this many frames)")
        # --- ğŸ‘† æ–°å¢ç»“æŸ ğŸ‘† ---
        env_pipeline_steps.append(
            TimeLimitProcessorStep(max_episode_steps=int(cfg.processor.reset.control_time_s * cfg.fps))
        )

    # Add gripper penalty processor if gripper config exists and enabled
    if cfg.processor.gripper is not None and cfg.processor.gripper.use_gripper:
        env_pipeline_steps.append(
            GripperPenaltyProcessorStep(
                penalty=cfg.processor.gripper.gripper_penalty,
                max_gripper_pos=cfg.processor.max_gripper_pos,
            )
        )

    env_pipeline_steps.append(AddBatchDimensionProcessorStep())
    env_pipeline_steps.append(DeviceProcessorStep(device=device))

    if (
        cfg.processor.reward_classifier is not None
        and cfg.processor.reward_classifier.pretrained_path is not None
    ):
        env_pipeline_steps.append(
            RewardClassifierProcessorStep(
                pretrained_path=cfg.processor.reward_classifier.pretrained_path,
                device=device,
                success_threshold=cfg.processor.reward_classifier.success_threshold,
                success_reward=cfg.processor.reward_classifier.success_reward,
                terminate_on_success=terminate_on_success,
            )
        )

    #åŠ¨æ€è§£æ URDF è·¯å¾„
    # æˆ‘ä»¬ç›´æ¥ä» teleop é…ç½®ä¸­è¯»å–è·¯å¾„ï¼Œå› ä¸ºé‚£é‡Œæ˜¯ä½ å®šä¹‰çš„çœŸå®ç¡¬ä»¶è·¯å¾„
    urdf_path = None
    if cfg.teleop and hasattr(cfg.teleop, "urdf_path"):
        raw_path = cfg.teleop.urdf_path
        # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼Œç¡®ä¿ pinocchio èƒ½æ‰¾åˆ°å®ƒ
        urdf_path = os.path.abspath(raw_path)
        print(f"ğŸ›¡ï¸ Safety Processor will use URDF: {urdf_path}")

    action_pipeline_steps = [
        AddTeleopActionAsComplimentaryDataStep(teleop_device=teleop_device),
        AddTeleopEventsAsInfoStep(teleop_device=teleop_device),
        InterventionActionProcessorStep(
            use_gripper=cfg.processor.gripper.use_gripper if cfg.processor.gripper is not None else False,
            terminate_on_success=terminate_on_success,
        ),
    ]

    # [æ–°å¢] å¦‚æœè·¯å¾„å­˜åœ¨ä¸”ç±»å·²åŠ è½½ï¼Œåˆ™æ·»åŠ å®‰å…¨æ‹¦æˆªå™¨
    if MKArmSafetyProcessorStep is not None and urdf_path is not None:
        action_pipeline_steps.append(
            MKArmSafetyProcessorStep(
                urdf_path=urdf_path, 
                min_z=0.220  # ä½ çš„å®‰å…¨é«˜åº¦é™åˆ¶
            )
        )
    else:
        print("âš ï¸ Skipping SafetyProcessor: URDF path missing or class not imported.")

    #
    # # Replace InverseKinematicsProcessor with new kinematic processors
    # if cfg.processor.inverse_kinematics is not None and kinematics_solver is not None:
    #     # Add EE bounds and safety processor
    #     inverse_kinematics_steps = [
    #         MapTensorToDeltaActionDictStep(
    #             use_gripper=cfg.processor.gripper.use_gripper if cfg.processor.gripper is not None else False
    #         ),
    #         MapDeltaActionToRobotActionStep(),
    #         EEReferenceAndDelta(
    #             kinematics=kinematics_solver,
    #             end_effector_step_sizes=cfg.processor.inverse_kinematics.end_effector_step_sizes,
    #             motor_names=motor_names,
    #             use_latched_reference=False,
    #             use_ik_solution=True,
    #         ),
    #         EEBoundsAndSafety(
    #             end_effector_bounds=cfg.processor.inverse_kinematics.end_effector_bounds,
    #         ),
    #         GripperVelocityToJoint(
    #             clip_max=cfg.processor.max_gripper_pos,
    #             speed_factor=1.0,
    #             discrete_gripper=True,
    #         ),
    #         InverseKinematicsRLStep(
    #             kinematics=kinematics_solver, motor_names=motor_names, initial_guess_current_joints=False
    #         ),
    #     ]
    #     action_pipeline_steps.extend(inverse_kinematics_steps)
    #     action_pipeline_steps.append(RobotActionToPolicyActionProcessorStep(motor_names=motor_names))

    return DataProcessorPipeline(
        steps=env_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
    ), DataProcessorPipeline(
        steps=action_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
    )


def step_env_and_process_transition(
    env: gym.Env,
    transition: EnvTransition,
    action: torch.Tensor,
    env_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
    action_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
) -> EnvTransition:
    """
    ä½¿ç”¨å¤„ç†å™¨ç®¡é“æ‰§è¡Œä¸€æ­¥ç¯å¢ƒäº¤äº’ã€‚
    åŒ…å« HIL-SERL æ ¸å¿ƒçš„çŠ¶æ€æœºé€»è¾‘ (Zeroing / Idle / Explore / Intervention)ã€‚
    """
    # Create action transition
    transition[TransitionKey.ACTION] = action
    
    raw_joints = env.get_raw_joint_positions() if hasattr(env, "get_raw_joint_positions") else {}
    if TransitionKey.OBSERVATION not in transition or not isinstance(transition[TransitionKey.OBSERVATION], dict):
        transition[TransitionKey.OBSERVATION] = {}
    transition[TransitionKey.OBSERVATION].update(raw_joints)

    # 1. æ‰§è¡Œ Action Pipeline (åŒ…å« InterventionProcessor å’Œ SafetyProcessor)
    # è¿™é‡Œçš„ processed_action å¯èƒ½æ˜¯ Policy åŠ¨ä½œï¼Œä¹Ÿå¯èƒ½æ˜¯è¢«æ›¿æ¢åçš„äººç±»åŠ¨ä½œ
    processed_action_transition = action_processor(transition)
    processed_action = processed_action_transition[TransitionKey.ACTION]
    print(f"processed_action shape: {processed_action.shape}")
    # å…‹éš†æœ€ç»ˆå†³å®šæ‰§è¡Œçš„åŠ¨ä½œ
    robot_action = processed_action.clone()
    print(f"robot_action shape0: {robot_action.shape}")
    # è·å–å½“å‰çœŸå®ä½ç½® (ç”¨äºæ»¤æ³¢åˆå§‹åŒ–å’Œå½’é›¶)
    joint_names = list(env.robot.bus.motors.keys()) 
    current_pos_list = [raw_joints[f"{name}.pos"] for name in joint_names]
    current_pos_tensor = torch.tensor(current_pos_list, device=robot_action.device, dtype=robot_action.dtype)
    
    # -------------------------------------------------------------------------
    # ğŸ® çŠ¶æ€æœºæ§åˆ¶é€»è¾‘ (State Machine Control)
    # -------------------------------------------------------------------------
    
    # 1. è·å–äº‹ä»¶ (ç”±åº•å±‚é©±åŠ¨å±‚å¤„ç†å¥½çš„æ„å›¾)
    info = processed_action_transition.get(TransitionKey.INFO, {})
    
    # A é”® (Start): å¯¹åº” SUCCESS äº‹ä»¶
    is_start_signal = info.get(TeleopEvents.SUCCESS, False)
    # X é”® (Reset): å¯¹åº” RERECORD äº‹ä»¶ (åº•å±‚å·²å¤„ç†é•¿æŒ‰)
    is_stop_signal = info.get(TeleopEvents.RERECORD_EPISODE, False)
    # RB é”® (Intervention): å¯¹åº” INTERVENTION äº‹ä»¶
    is_intervention = info.get(TeleopEvents.IS_INTERVENTION, False)

    # 2. çŠ¶æ€åˆ‡æ¢é€»è¾‘ (App å±‚å†³ç­–)
    
    # [IDLE -> EXPLORE] æ”¶åˆ°å¯åŠ¨ä¿¡å· (A)
    if is_start_signal:
        if env.rl_mode != "EXPLORE":
            env.rl_mode = "EXPLORE"
            print("\nğŸš€ [System] ACTIVATED: Policy Exploration Started! (A pressed)")
            # è¿™é‡Œå¯ä»¥é‡ç½®å¹³æ»‘å™¨ï¼Œé˜²æ­¢ä¸Šæ¬¡çš„æ®‹ç•™
            env.last_policy_action = None

    # [ANY -> ZEROING] æ”¶åˆ°åœæ­¢ä¿¡å· (X)
    if is_stop_signal:
        if env.rl_mode != "ZEROING":
            env.rl_mode = "ZEROING"
            print("\nğŸ›‘ [System] STOPPED: Returning to ZERO... (X pressed)")

    print(f"robot_action shape1: {robot_action.shape}")

    # 3. æ ¹æ®å½“å‰æ¨¡å¼ä¿®æ­£ robot_action
    # [Case 1: äººå·¥ä»‹å…¥ä¸­]
    # æ— è®ºå¤„äºä»€ä¹ˆæ¨¡å¼ï¼Œåªè¦æŒ‰ä¸‹äº†ä»‹å…¥é”®ï¼Œå°±å¬äººç±»çš„ã€‚
    # å…³é”®ï¼šåœ¨è¿™é‡ŒåŒæ­¥ Policy çš„å¹³æ»‘å™¨è®°å¿†ï¼Œä¿è¯æ¾æ‰‹æ—¶ 0 è·³å˜ã€‚
    if is_intervention:
        # robot_action å·²ç»æ˜¯äººç±»åŠ¨ä½œäº† (ç”± action_processor å¤„ç†è¿‡)
        # æˆ‘ä»¬æŠŠå½“å‰çœŸå®ä½ç½® (æˆ–è€…äººç±»åŠ¨ä½œ) å–‚ç»™æ»¤æ³¢å™¨çš„è®°å¿†
        # è¿™æ ·æ»¤æ³¢å™¨ä¸‹ä¸€æ¬¡è®¡ç®—æ—¶ï¼Œ"ä¸Šä¸€æ¬¡åŠ¨ä½œ" å°±æ˜¯ç°åœ¨è¿™ä¸ªä½ç½®
        if robot_action.ndim == 2:
            env.last_policy_action = robot_action[:, :6].clone()
        else:
            env.last_policy_action = robot_action[:6].clone()

    # [Case 2: è‡ªåŠ¨å½’é›¶æ¨¡å¼]
    elif env.rl_mode == "ZEROING":
        ZERO_SPEED = 0.05
        target = torch.zeros_like(current_pos_tensor)
        # ä»…å½’é›¶æ‰‹è‡‚(å‰6è½´)ï¼Œå¤¹çˆªä¿æŒä¸åŠ¨
        if robot_action.ndim == 2: # [1, 7]
            target = target.unsqueeze(0)
            target[:, 6] = current_pos_tensor[6] 
            delta = target[:, :6] - current_pos_tensor[:6]
            delta = torch.clamp(delta, -ZERO_SPEED, ZERO_SPEED)
            robot_action[:, :6] = current_pos_tensor[:6] + delta
            
            if torch.abs(current_pos_tensor[:6]).max() < 0.05:
                env.rl_mode = "IDLE"
                print("âœ… [System] Zeroed. Entering IDLE mode.")
        else:
            target[6] = current_pos_tensor[6]
            delta = target[:6] - current_pos_tensor[:6]
            delta = torch.clamp(delta, -ZERO_SPEED, ZERO_SPEED)
            robot_action[:6] = current_pos_tensor[:6] + delta
            
            if torch.abs(current_pos_tensor[:6]).max() < 0.05:
                env.rl_mode = "IDLE"
                print("âœ… [System] Zeroed. Entering IDLE mode.")

    # [Case 3: IDLE æ¨¡å¼] é”æ­»ä¸åŠ¨
    elif env.rl_mode == "IDLE":
        # å¼ºåˆ¶åŠ¨ä½œç­‰äºå½“å‰ä½ç½®
        if robot_action.ndim == 2:
            robot_action = current_pos_tensor.unsqueeze(0)
        else:
            robot_action = current_pos_tensor

    # [Case 4: EXPLORE æ¨¡å¼] Policy æ§åˆ¶ (å¸¦æ»¤æ³¢å’Œè½¯é™ä½)
    elif env.rl_mode == "EXPLORE":
        # # å¦‚æœæ²¡æœ‰ä»‹å…¥ï¼Œè¿™é‡Œçš„ robot_action æ˜¯ Policy çš„åŸå§‹è¾“å‡º
        
        # POLICY_MAX_STEP = 0.04
        # EMA_ALPHA = 0.2
        
        # # æå–æ•°æ®
        # arm_target = None
        # arm_current = None
        # if robot_action.ndim == 2: 
        #     arm_target = robot_action[:, :6] 
        #     arm_current = current_pos_tensor[:6].unsqueeze(0)
        # elif robot_action.ndim == 1:
        #     arm_target = robot_action[:6]
        #     arm_current = current_pos_tensor[:6]
            
        # if arm_target is not None:
        #     # [æ»¤æ³¢]
        #     last_action = env.last_policy_action
        #     if last_action is None: last_action = arm_current.clone()
            
        #     # ç»´åº¦å¯¹é½
        #     if last_action.ndim != arm_target.ndim:
        #         if arm_target.ndim == 2: last_action = last_action.unsqueeze(0)
            
        #     # EMA å…¬å¼
        #     arm_target_smoothed = EMA_ALPHA * arm_target + (1 - EMA_ALPHA) * last_action
        #     env.last_policy_action = arm_target_smoothed.detach()

        #     # [è½¯é™ä½] Policy Safe Limits (åœ¨ gym_manipulator é¡¶éƒ¨å®šä¹‰)
        #     # æ³¨æ„ï¼šè¿™é‡Œçš„é™ä½æ˜¯ç»™ Policy çš„"æ´»åŠ¨èŒƒå›´"ï¼Œå¯ä»¥æ¯” SafetyProcessor çš„ç¡¬é™ä½æ›´ä¿å®ˆ
        #     for i in range(6):
        #         min_lim, max_lim = POLICY_SAFE_LIMITS.get(i, (-3.14, 3.14))
        #         if robot_action.ndim == 2:
        #             arm_target_smoothed[:, i] = torch.clamp(arm_target_smoothed[:, i], min_lim, max_lim)
        #         else:
        #             arm_target_smoothed[i] = torch.clamp(arm_target_smoothed[i], min_lim, max_lim)

        #     # [é™é€Ÿ]
        #     delta = arm_target_smoothed - arm_current
        #     delta_clipped = torch.clamp(delta, -POLICY_MAX_STEP, POLICY_MAX_STEP)
            
        #     if robot_action.ndim == 2:
        #         robot_action[:, :6] = arm_current + delta_clipped
        #     else:
        #         robot_action[:6] = arm_current + delta_clipped

        # å¦‚æœæ²¡æœ‰ä»‹å…¥ï¼Œè¿™é‡Œçš„ robot_action æ˜¯ Policy çš„åŸå§‹è¾“å‡º (èŒƒå›´ -1 ~ 1)
        
        # === ä¿®æ”¹å¼€å§‹: æ”¹ä¸º Delta Control ===
        
        # å®šä¹‰ Action Scale: Policy è¾“å‡º 1.0 ä»£è¡¨ä¸€æ­¥ç§»åŠ¨å¤šå°‘å¼§åº¦ï¼Ÿ
        # å»ºè®®è®¾å°ä¸€ç‚¹ï¼Œä¿è¯åŠ¨ä½œç»†è…»ã€‚ä¾‹å¦‚ 0.05 rad/step
        ACTION_SCALE = 0.05 
        
        # æå–æ•°æ®
        policy_output_delta = None
        arm_current = None
        
        if robot_action.ndim == 2: 
            policy_output_delta = robot_action[:, :6] 
            arm_current = current_pos_tensor[:6].unsqueeze(0)
        elif robot_action.ndim == 1:
            policy_output_delta = robot_action[:6]
            arm_current = current_pos_tensor[:6]
            
        if policy_output_delta is not None:
            # 1. è®¡ç®—ç›®æ ‡ä½ç½®: Target = Current + (Policy_Output * Scale)
            # è¿™æ ·å¦‚æœ Policy è¾“å‡º 0 (æœªçŸ¥/çŠ¹è±«)ï¼Œæœºæ¢°è‡‚å°±ä¼šåœåœ¨åŸåœ°ï¼Œè€Œä¸æ˜¯å½’é›¶
            delta = policy_output_delta * ACTION_SCALE
            
            # 2. [å¯é€‰] ä»ç„¶ä¿ç•™å¹³æ»‘æ»¤æ³¢ï¼Œä½†ä½œç”¨åœ¨ Delta ä¸Šå¯èƒ½å¯¼è‡´æ»åï¼Œ
            # å»ºè®®ç›´æ¥ä½œç”¨åœ¨æœ€ç»ˆ Target ä¸Šï¼Œæˆ–è€…åœ¨ Delta Control ä¸‹æš‚æ—¶å»æ‰ EMAï¼Œ
            # å› ä¸º SAC æœ¬èº«è¾“å‡ºå°±æ˜¯è¿ç»­å˜åŒ–çš„ã€‚ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆè®¡ç®—å‡º Targetã€‚
            arm_target = arm_current + delta
            
            # æ‰‹åŠ¨è°ƒç”¨ SafetyProcessor æ£€æŸ¥è¿™ä¸ª arm_target (ç»å¯¹å€¼)
            # åªæœ‰å½“ pipeline é‡Œé…ç½®äº† safety processor æ—¶æ‰æ‰§è¡Œ
            safety_step = None
            for step in action_processor.steps:
                if isinstance(step, MKArmSafetyProcessorStep):
                    safety_step = step
                    break
                    
            if safety_step is not None:
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ transition ä¸“é—¨ç”¨äºæ£€æŸ¥ Target
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éª— Processor è¯´è¿™æ˜¯ actionï¼Œå®é™…ä¸Šç»™çš„æ˜¯ç»å¯¹ä½ç½® target
                check_transition = transition.copy()
                check_transition[TransitionKey.ACTION] = arm_target  # å…³é”®ï¼šä¼ å…¥ç»å¯¹ä½ç½®ï¼
                
                # å¤ç”¨ safety_processor çš„é€»è¾‘
                # æ³¨æ„ï¼šéœ€è¦ä¿®æ”¹ safety_processor è®©å®ƒæ”¯æŒ return bool æˆ–æŠ›å¼‚å¸¸ï¼Œ
                # æˆ–è€…æˆ‘ä»¬ç›´æ¥è§‚å¯Ÿå®ƒæ˜¯å¦ä¿®æ”¹äº† action (å›æ»š)
                
                # æ›´ç®€å•çš„åšæ³•ï¼šç›´æ¥å¤ç”¨ safety_processor é‡Œçš„æ ¸å¿ƒæ£€æŸ¥å‡½æ•°
                # ä½†ç”±äºå®ƒæ˜¯ ProcessorStepï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨å®ƒ
                result_transition = safety_step(check_transition)
                
                # å¦‚æœ SafetyProcessor å‘ç°è¿è§„ï¼Œå®ƒä¼šæŠŠ action æ›¿æ¢å› last_safe_action
                # æˆ‘ä»¬æ£€æŸ¥ result_transition['action'] æ˜¯å¦å˜äº†ï¼Œæˆ–è€…æ˜¯å¦ç­‰äº arm_target
                safe_action = result_transition[TransitionKey.ACTION]
                # ä½¿ç”¨ ... (Ellipsis) å¯ä»¥åŒæ—¶å¤„ç† 1D [7] å’Œ 2D [B, 7] çš„æƒ…å†µ
                safe_action_6d = safe_action[..., :6]
                arm_target = safe_action_6d

            # 3. [è½¯é™ä½] Policy Safe Limits
            # é™åˆ¶ Target ä¸è¦è¶…å‡ºå®‰å…¨èŒƒå›´
            for i in range(6):
                min_lim, max_lim = POLICY_SAFE_LIMITS.get(i, (-3.14, 3.14))
                if robot_action.ndim == 2:
                    arm_target[:, i] = torch.clamp(arm_target[:, i], min_lim, max_lim)
                else:
                    arm_target[i] = torch.clamp(arm_target[i], min_lim, max_lim)

            # 4. [æœ€ç»ˆèµ‹å€¼]
            #æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™ squeeze æ‰å¤šä½™çš„ batch ç»´åº¦
            # # æƒ…å†µ A: robot_action æ˜¯ 1D [7], ä½† arm_target æ˜¯ 2D [1, 6]
            # # è§£å†³: æŠŠ arm_target å‹æ‰æˆ [6]
            # if robot_action.ndim == 1 and arm_target.ndim == 2:
            #     arm_target = arm_target.squeeze(0)
            # # æƒ…å†µ B: robot_action æ˜¯ 2D [1, 7], ä½† arm_target æ˜¯ 1D [6]
            # # è§£å†³: æŠŠ arm_target å‡ç»´æˆ [1, 6]
            # elif robot_action.ndim == 2 and arm_target.ndim == 1:
            #     arm_target = arm_target.unsqueeze(0)

            # å› ä¸ºæˆ‘ä»¬æ˜¯åŸºäº Current ç®—çš„ Deltaï¼Œæ‰€ä»¥ä¸éœ€è¦å†åšé¢å¤–çš„é™é€Ÿ (ACTION_SCALE å°±æ˜¯é™é€Ÿ)
            if robot_action.ndim == 2:
                robot_action[:, :6] = arm_target
            else:
                robot_action[:6] = arm_target

            print(f"robot_action shape2: {robot_action.shape}")
            if env.current_step % 5 == 0: # æé«˜é¢‘ç‡è§‚å¯Ÿ
                # è·å–å‰3è½´çš„æ•°æ® (é€šå¸¸æ’ç›¸æœºçš„æ˜¯ Base, Shoulder æˆ– Elbow)
                j0_curr = arm_current.squeeze()[0].item()
                j0_targ = arm_target.squeeze()[0].item()
                
                # æ‰“å°æ ¼å¼: [Step] J0_Curr -> J0_Target (Policy_Delta)
                # å¦‚æœ J0 è§’åº¦å¾ˆå¤§ï¼Œä¸”è¿˜åœ¨å¾€å¤§å˜ï¼Œå°±å¾ˆå±é™©
                raw_d = policy_output_delta.squeeze()[0].item()
                print(f"âš ï¸ [Step {env.current_step}] J0(Base): {j0_curr:.3f} -> {j0_targ:.3f} (Delta_Raw: {raw_d:.2f})")
                
                # æ£€æŸ¥æ˜¯å¦è´´è¿‘é™ä½è¾¹ç•Œ
                for i in range(3): # åªæ£€æŸ¥å‰3ä¸ªä¸»è¦å…³èŠ‚
                    min_lim, max_lim = POLICY_SAFE_LIMITS.get(i, (-99, 99))
                    curr_val = arm_current.squeeze()[i].item()
                    if curr_val < min_lim + 0.05 or curr_val > max_lim - 0.05:
                        print(f"  ğŸš¨ DANGER ZONE: Joint {i} at {curr_val:.3f} is near limit {min_lim}~{max_lim}!")
    # -------------------------------------------------------------------------
    # æ ¼å¼è½¬æ¢å› Numpy å‘é€ç»™ Robot
    if isinstance(robot_action, torch.Tensor):
        robot_action = robot_action.cpu().numpy()
    
    if robot_action.ndim > 1:
        robot_action = robot_action.squeeze(0)

    print(f"robot_action shape: {robot_action.shape}")
    # å‘é€åŠ¨ä½œ
    obs, reward, terminated, truncated, info = env.step(robot_action)

    # ç»„è£… Transition (å­˜å…¥ Buffer)
    # æ³¨æ„ï¼šBuffer ä¸­å­˜å‚¨çš„æ˜¯ processed_action (å³ Policy åŸå§‹è¾“å‡º æˆ–è€… äººç±»åŠ¨ä½œ)
    # è€Œä¸æ˜¯ç»è¿‡æ»¤æ³¢ã€é™é€Ÿåçš„ robot_actionã€‚è¿™ç¬¦åˆ SERL é€»è¾‘ï¼šå­˜æ„å›¾ï¼Œè€Œéæ‰§è¡Œç»“æœã€‚
    reward = reward + processed_action_transition[TransitionKey.REWARD]
    terminated = terminated or processed_action_transition[TransitionKey.DONE]
    truncated = truncated or processed_action_transition[TransitionKey.TRUNCATED]
    complementary_data = processed_action_transition[TransitionKey.COMPLEMENTARY_DATA].copy()
    
    #new_info = processed_action_transition[TransitionKey.INFO].copy()
    #new_info.update(info)
    env_info = info.copy()
    # ç§»é™¤ env_info ä¸­çš„ç¡¬ç¼–ç  Falseï¼Œé¿å…è¦†ç›–
    if TeleopEvents.IS_INTERVENTION in env_info:
        del env_info[TeleopEvents.IS_INTERVENTION]
    new_info = processed_action_transition[TransitionKey.INFO].copy()
    new_info.update(env_info)    

    new_transition = create_transition(
        observation=obs,
        action=processed_action, 
        reward=reward,
        done=terminated,
        truncated=truncated,
        info=new_info,
        complementary_data=complementary_data,
    )
    new_transition = env_processor(new_transition)

    return new_transition

# def step_env_and_process_transition(
#     env: gym.Env,
#     transition: EnvTransition,
#     action: torch.Tensor,
#     env_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
#     action_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
# ) -> EnvTransition:
#     """
#     ä½¿ç”¨å¤„ç†å™¨ç®¡é“æ‰§è¡Œä¸€æ­¥ç¯å¢ƒäº¤äº’ã€‚
#     """
#     # Create action transition
#     transition[TransitionKey.ACTION] = action
    
#     raw_joints = env.get_raw_joint_positions() if hasattr(env, "get_raw_joint_positions") else {}
#     if TransitionKey.OBSERVATION not in transition or not isinstance(transition[TransitionKey.OBSERVATION], dict):
#         transition[TransitionKey.OBSERVATION] = {}
#     transition[TransitionKey.OBSERVATION].update(raw_joints)

#     processed_action_transition = action_processor(transition)
#     processed_action = processed_action_transition[TransitionKey.ACTION]

#     # ä½¿ç”¨ clone() åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ç›´æ¥ä¿®æ”¹ Buffer ä¸­å­˜å‚¨çš„åŸå§‹ Policy åŠ¨ä½œ
#     robot_action = processed_action.clone()
    
#     # =================================================================
#     # ğŸ›¡ï¸ ä¸‰é‡å®‰å…¨é€»è¾‘: æ»¤æ³¢(Smoothing) + é™ä½(Safe Zone) + é™é€Ÿ(Speed Limit)
#     # =================================================================
    
#     # 1. æ£€æŸ¥æ˜¯å¦æœ‰äººå·¥ä»‹å…¥
#     is_intervention = False
#     if TransitionKey.INFO in processed_action_transition:
#         info = processed_action_transition[TransitionKey.INFO]
#         is_rb_pressed = info.get(TeleopEvents.IS_INTERVENTION, False)
#         is_success_pressed = info.get(TeleopEvents.SUCCESS, False)
#         is_failure_pressed = info.get(TeleopEvents.FAILURE, False)
#         is_rerecord_pressed = info.get(TeleopEvents.RERECORD_EPISODE, False)
        
#         if is_success_pressed: print("ğŸ’¡ User Signal: SUCCESS (Y)")
#         if is_rerecord_pressed: print("ğŸ’¡ User Signal: RERECORD/RESET (X)")
            
#         is_intervention = is_rb_pressed or is_success_pressed or is_failure_pressed or is_rerecord_pressed
    
#     # å¦‚æœä»‹å…¥äº†ï¼Œæ¸…ç©º Policy å¹³æ»‘å™¨çš„è®°å¿†ï¼Œé¿å…ä¸‹æ¬¡æ¥ç®¡æ—¶è·³å˜
#     if is_intervention:
#         env.last_policy_action = None

#     # 2. å¦‚æœæ˜¯ Policy æ§åˆ¶ (éä»‹å…¥çŠ¶æ€)ï¼Œæ‰§è¡Œå¹³æ»‘å’Œé™åˆ¶
#     if not is_intervention and isinstance(robot_action, torch.Tensor):
#         POLICY_MAX_STEP = 0.04  # é€Ÿåº¦ä¸Šé™
#         EMA_ALPHA = 0.2         # å¹³æ»‘ç³»æ•° (0.1~1.0)ï¼Œè¶Šå°è¶Šé¡ºæ»‘ä½†å»¶è¿Ÿè¶Šé«˜
        
#         joint_names = list(env.robot.bus.motors.keys()) 
#         current_pos_list = [raw_joints[f"{name}.pos"] for name in joint_names]
        
#         current_pos_tensor = torch.tensor(
#             current_pos_list, 
#             device=robot_action.device, 
#             dtype=robot_action.dtype
#         )
        
#         # [A] æå–å…³èŠ‚ç›®æ ‡ & å¢åŠ  Batch ç»´åº¦
#         arm_target = None
#         arm_current = None
        
#         if robot_action.ndim == 2: # [Batch, 7]
#             arm_target = robot_action[:, :6] 
#             arm_current = current_pos_tensor[:6].unsqueeze(0)
#         elif robot_action.ndim == 1: # [7]
#             arm_target = robot_action[:6]
#             arm_current = current_pos_tensor[:6]
            
#         if arm_target is not None:
#             # [B] EMA å¹³æ»‘æ»¤æ³¢ (Anti-Jitter)
#             # ----------------------------------------------------
#             last_action = env.last_policy_action
            
#             # å¦‚æœæ²¡æœ‰å†å²è®°å½•ï¼ˆåˆšå¼€å§‹æˆ–åˆšç»“æŸä»‹å…¥ï¼‰ï¼Œç”¨å½“å‰çœŸå®ä½ç½®åˆå§‹åŒ–
#             # è¿™æ ·ä¿è¯ä»é™æ­¢å¼€å§‹å¯åŠ¨ï¼Œä¸ä¼šçªå˜
#             if last_action is None:
#                 last_action = arm_current.clone()
            
#             # ç¡®ä¿ç»´åº¦åŒ¹é… (å¤„ç† Batch å¹¿æ’­)
#             if last_action.ndim != arm_target.ndim:
#                 if arm_target.ndim == 2: last_action = last_action.unsqueeze(0)
                
#             # æ‰§è¡Œæ»¤æ³¢å…¬å¼: Smoothed = alpha * New + (1-alpha) * Old
#             arm_target_smoothed = EMA_ALPHA * arm_target + (1 - EMA_ALPHA) * last_action
            
#             # æ›´æ–°è®°å¿†
#             env.last_policy_action = arm_target_smoothed.detach() # detaché˜²æ­¢æ¢¯åº¦ç´¯ç§¯
#             # ----------------------------------------------------

#             # [C] Policy å®‰å…¨å±‹ (ä½¿ç”¨å¹³æ»‘åçš„ç›®æ ‡)
#             for i in range(6):
#                 min_lim, max_lim = POLICY_SAFE_LIMITS.get(i, (-3.14, 3.14))
#                 if robot_action.ndim == 2:
#                     arm_target_smoothed[:, i] = torch.clamp(arm_target_smoothed[:, i], min_lim, max_lim)
#                 else:
#                     arm_target_smoothed[i] = torch.clamp(arm_target_smoothed[i], min_lim, max_lim)

#             # [D] é€Ÿåº¦é™åˆ¶ (åŸºäºå¹³æ»‘åçš„ç›®æ ‡è®¡ç®— Delta)
#             delta = arm_target_smoothed - arm_current
#             delta_clipped = torch.clamp(delta, -POLICY_MAX_STEP, POLICY_MAX_STEP)
            
#             # [E] å†™å› robot_action
#             if robot_action.ndim == 2:
#                 robot_action[:, :6] = arm_current + delta_clipped
#             else:
#                 robot_action[:6] = arm_current + delta_clipped

#     # =================================================================

#     if isinstance(robot_action, torch.Tensor):
#         robot_action = robot_action.cpu().numpy()
    
#     if robot_action.ndim > 1:
#         robot_action = robot_action.squeeze(0)

#     obs, reward, terminated, truncated, info = env.step(robot_action)

#     reward = reward + processed_action_transition[TransitionKey.REWARD]
#     terminated = terminated or processed_action_transition[TransitionKey.DONE]
#     truncated = truncated or processed_action_transition[TransitionKey.TRUNCATED]
#     complementary_data = processed_action_transition[TransitionKey.COMPLEMENTARY_DATA].copy()
#     new_info = processed_action_transition[TransitionKey.INFO].copy()
#     new_info.update(info)

#     new_transition = create_transition(
#         observation=obs,
#         action=processed_action,
#         reward=reward,
#         done=terminated,
#         truncated=truncated,
#         info=new_info,
#         complementary_data=complementary_data,
#     )
    
#     new_transition = env_processor(new_transition)

#     return new_transition



def control_loop(
    env: gym.Env,
    env_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
    action_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
    teleop_device: Teleoperator,
    cfg: GymManipulatorConfig,
) -> None:
    dt = 1.0 / cfg.env.fps

    print(f"Starting control loop at {cfg.env.fps} FPS")
    print("Controls:")
    print("- Press [A]: START Exploration")  # <--- æ”¹è¿™ä¸ª
    print("- Long Press [X] (1s): STOP & Return to ZERO")
    print("- Hold [RB]: Manual Intervention")
    print(f"Current Mode: {env.rl_mode}")

    obs, info = env.reset()
    complementary_data = (
        {"raw_joint_positions": info.pop("raw_joint_positions")} if "raw_joint_positions" in info else {}
    )
    env_processor.reset()
    action_processor.reset()

    transition = create_transition(observation=obs, info=info, complementary_data=complementary_data)
    transition = env_processor(data=transition)

    use_gripper = cfg.env.processor.gripper.use_gripper if cfg.env.processor.gripper is not None else True

    dataset = None
    if cfg.mode == "record":
        action_features = teleop_device.action_features
        features = {
            ACTION: action_features,
            REWARD: {"dtype": "float32", "shape": (1,), "names": None},
            DONE: {"dtype": "bool", "shape": (1,), "names": None},
        }
        if use_gripper:
            features["complementary_info.discrete_penalty"] = {
                "dtype": "float32",
                "shape": (1,),
                "names": ["discrete_penalty"],
            }

        for key, value in transition[TransitionKey.OBSERVATION].items():
            if key == OBS_STATE:
                features[key] = {
                    "dtype": "float32",
                    "shape": value.squeeze(0).shape,
                    "names": None,
                }
            if "image" in key:
                features[key] = {
                    "dtype": "video",
                    "shape": value.squeeze(0).shape,
                    "names": ["channels", "height", "width"],
                }

        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.env.fps,
            root=cfg.dataset.root,
            use_videos=True,
            image_writer_threads=4,
            image_writer_processes=0,
            features=features,
        )

    episode_idx = 0
    episode_step = 0
    episode_success_frames = 0
    episode_start_time = time.perf_counter()

    current_joints = env.get_raw_joint_positions()
    joint_names = list(env.robot.bus.motors.keys())
    neutral_action = torch.tensor([current_joints[f"{k}.pos"] for k in joint_names], dtype=torch.float32)

    while episode_idx < cfg.dataset.num_episodes_to_record:
        step_start_time = time.perf_counter()

        if not isinstance(neutral_action, torch.Tensor):
             neutral_action = torch.from_numpy(neutral_action).float()

        transition = step_env_and_process_transition(
            env=env,
            transition=transition,
            action=neutral_action,
            env_processor=env_processor,
            action_processor=action_processor,
        )

        # [Anti-Windup Logic] æ¯æ¬¡å¾ªç¯åï¼Œé‡ç½® neutral_action ä¸ºå½“å‰çœŸå®ä½ç½®
        # è§£å†³ Policy æˆ– æ‰‹æŸ„ æ“ä½œåçš„ä½ç½®åå·®
        obs_dict = transition[TransitionKey.OBSERVATION]
        current_joint_vals = []
        for name in joint_names:
             key = f"{name}.pos"
             val = obs_dict[key]
             if hasattr(val, "item"):
                 val = val.item()
             current_joint_vals.append(val)
        neutral_action = torch.tensor(current_joint_vals, dtype=torch.float32)

        # Print Info
        reward_val = transition[TransitionKey.REWARD]
        reward_val = reward_val.item() if hasattr(reward_val, "item") else reward_val
        print(f"Epi: {episode_idx} | Reward: {reward_val:.4f} | Steps: {episode_step}", end="\r")

        terminated = transition.get(TransitionKey.DONE, False)
        truncated = transition.get(TransitionKey.TRUNCATED, False)

        if reward_val > 0.0:
            episode_success_frames += 1

        if cfg.mode == "record":
            observations = {
                k: v.squeeze(0).cpu()
                for k, v in transition[TransitionKey.OBSERVATION].items()
                if isinstance(v, torch.Tensor)
            }
            action_to_record = transition[TransitionKey.COMPLEMENTARY_DATA].get(
                "teleop_action", transition[TransitionKey.ACTION]
            )
            frame = {
                **observations,
                ACTION: action_to_record.cpu(),
                REWARD: np.array([transition[TransitionKey.REWARD]], dtype=np.float32),
                DONE: np.array([terminated or truncated], dtype=bool),
            }
            if use_gripper:
                discrete_penalty = transition[TransitionKey.COMPLEMENTARY_DATA].get("discrete_penalty", 0.0)
                frame["complementary_info.discrete_penalty"] = np.array([discrete_penalty], dtype=np.float32)

            if dataset is not None:
                frame["task"] = cfg.dataset.task
                dataset.add_frame(frame)

        episode_step += 1

        if terminated or truncated:
            episode_time = time.perf_counter() - episode_start_time
            # æ£€æµ‹æ˜¯å¦æ˜¯å› ä¸ºç”¨æˆ·æ‰‹åŠ¨é‡ç½®
            is_rerecord = transition[TransitionKey.INFO].get(TeleopEvents.RERECORD_EPISODE, False)
            if is_rerecord:
                logging.info(f"\nğŸ”„ Episode {episode_idx} RESET by USER (Button X). Reward={reward_val}")
            else:
                logging.info(
                    f"\nâœ… Episode {episode_idx} finished. Steps: {episode_step}. Reward: {reward_val}"
                )

            episode_step = 0
            episode_success_frames = 0
            episode_idx += 1

            if dataset is not None:
                if is_rerecord:
                    logging.info(f"Re-recording episode {episode_idx}")
                    dataset.clear_episode_buffer()
                    episode_idx -= 1
                else:
                    logging.info(f"Saving episode {episode_idx}")
                    dataset.save_episode()

            obs, info = env.reset()
            env_processor.reset()
            action_processor.reset()

            current_joints = env.get_raw_joint_positions()
            neutral_action = torch.tensor([current_joints[f"{k}.pos"] for k in joint_names], dtype=torch.float32)

            transition = create_transition(observation=obs, info=info)
            transition = env_processor(transition)
            episode_start_time = time.perf_counter()

        busy_wait(dt - (time.perf_counter() - step_start_time))

    if dataset is not None and cfg.dataset.push_to_hub:
        logging.info("Pushing dataset to hub")
        dataset.push_to_hub()


def replay_trajectory(
    env: gym.Env, action_processor: DataProcessorPipeline, cfg: GymManipulatorConfig
) -> None:
    """Replay recorded trajectory on robot environment."""
    assert cfg.dataset.replay_episode is not None, "Replay episode must be provided for replay"

    dataset = LeRobotDataset(
        cfg.dataset.repo_id,
        root=cfg.dataset.root,
        episodes=[cfg.dataset.replay_episode],
        download_videos=False,
    )
    episode_frames = dataset.hf_dataset.filter(lambda x: x["episode_index"] == cfg.dataset.replay_episode)
    actions = episode_frames.select_columns(ACTION)

    _, info = env.reset()

    for action_data in actions:
        start_time = time.perf_counter()
        transition = create_transition(
            observation=env.get_raw_joint_positions() if hasattr(env, "get_raw_joint_positions") else {},
            action=action_data[ACTION],
        )
        transition = action_processor(transition)
        env.step(transition[TransitionKey.ACTION])
        busy_wait(1 / cfg.env.fps - (time.perf_counter() - start_time))


@parser.wrap()
def main(cfg: GymManipulatorConfig) -> None:
    """Main entry point for gym manipulator script."""
    env, teleop_device = make_robot_env(cfg.env)
    env_processor, action_processor = make_processors(env, teleop_device, cfg.env, cfg.device)

    print("Environment observation space:", env.observation_space)
    print("Environment action space:", env.action_space)
    print("Environment processor:", env_processor)
    print("Action processor:", action_processor)

    if cfg.mode == "replay":
        replay_trajectory(env, action_processor, cfg)
        exit()

    control_loop(env, env_processor, action_processor, teleop_device, cfg)


if __name__ == "__main__":
    main()
